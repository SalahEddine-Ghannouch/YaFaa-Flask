{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a spark session\n",
    "spark = SparkSession.builder.appName(\"DataCleaning\").getOrCreate()\n",
    "\n",
    "# Get the path of the foler containing all our Data\n",
    "root_dir = '/home/hadoop/AllData/Fixtures' # Needs to be changed\n",
    "\n",
    "# Define a function that returns only the league we need\n",
    "def league_id_dir(folder_name,id):\n",
    "    return folder_name.startswith(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Schema of the DataFrame\n",
    "Data_schema =StructType([\n",
    "StructField('fixture_id',IntegerType(),True),  \n",
    "StructField('fixture_referee',StringType(),True),  \n",
    "StructField('fixture_timezone',StringType(),True),  \n",
    "StructField('fixture_date',StringType(),True),  \n",
    "StructField('fixture_timestamp',IntegerType(),True),  \n",
    "StructField('fixture_periods_first',IntegerType(),True),  \n",
    "StructField('fixture_periods_second',IntegerType(),True),\n",
    "StructField('venue_id',IntegerType(),True), \n",
    "StructField('venue_name',StringType(),True),  \n",
    "StructField('venue_city',StringType(),True),  \n",
    "StructField('status_long',StringType(),True),  \n",
    "StructField('status_short',StringType(),True),  \n",
    "StructField('status_elapsed',IntegerType(),True),  \n",
    "StructField('league_id',IntegerType(),True),  \n",
    "StructField('league_name',StringType(),True),  \n",
    "StructField('league_country',StringType(),True),  \n",
    "StructField('league_logo',StringType(),True),  \n",
    "StructField('league_flag',StringType(),True),  \n",
    "StructField('league_season',IntegerType(),True),  \n",
    "StructField('league_round',StringType(),True),  \n",
    "StructField('teams_home_id',IntegerType(),True),  \n",
    "StructField('teams_home_name',StringType(),True),  \n",
    "StructField('teams_home_logo',StringType(),True),  \n",
    "StructField('teams_home_winner',BooleanType(),True),  \n",
    "StructField('teams_away_id',IntegerType(),True),  \n",
    "StructField('teams_away_name',StringType(),True),  \n",
    "StructField('teams_away_logo',StringType(),True),  \n",
    "StructField('teams_away_winner',BooleanType(),True),  \n",
    "StructField('goals_home',IntegerType(),True),  \n",
    "StructField('goals_away',IntegerType(),True),  \n",
    "StructField('score_halftime_home',IntegerType(),True),  \n",
    "StructField('score_halftime_away',IntegerType(),True),  \n",
    "StructField('score_fulltime_home',IntegerType(),True),  \n",
    "StructField('score_fulltime_away',IntegerType(),True),  \n",
    "StructField('score_extratime_home',IntegerType(),True),  \n",
    "StructField('score_extratime_away',IntegerType(),True),  \n",
    "StructField('score_penalty_home',IntegerType(),True),  \n",
    "StructField('score_penalty_away',IntegerType(),True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the path of the foler containing all our Data\n",
    "root_dir = '/home/hadoop/AllData' # Needs to be changed\n",
    "# List of the leagues IDs\n",
    "leagues = ['39', '135', '140', '78', '61']\n",
    "# Create an empty DataFrame\n",
    "emp_RDD = spark.sparkContext.emptyRDD()\n",
    "# Iterate throw all the leagues IDs\n",
    "for i in leagues:\n",
    "    # Get the league file\n",
    "    directories = [dir for dir in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, dir)) and league_id_dir(dir,i)]\n",
    "    combined_df = spark.createDataFrame(data = emp_RDD, schema=Data_schema)\n",
    "    for folder in directories:\n",
    "        folder_path = os.path.join(root_dir, folder)\n",
    "        files = os.listdir(folder_path)\n",
    "        csv_files = [file for file in files if file.endswith('.csv')]\n",
    "        for csv_file in csv_files:\n",
    "            file_path = os.path.join(folder_path, csv_file)\n",
    "            df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "            combined_df = combined_df.union(df)\n",
    "    combined_df = df.repartition(1)\n",
    "    combined_df.write.csv('Fixtures/'+str(i)+'data.csv', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Murging the data by season and by league into one csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the path of the foler containing all our Data\n",
    "root_dir = '/home/hadoop/AllData' # Needs to be changed\n",
    "\n",
    "# Define a function that returns only the league we need\n",
    "def league_id_dir(folder_name,id):\n",
    "    return folder_name.startswith(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the path of the foler containing all our Data\n",
    "root_dir = '/home/hadoop/AllData' # Needs to be changed\n",
    "\n",
    "# List of the leagues IDs\n",
    "leagues = ['39', '135', '140', '78', '61']\n",
    "\n",
    "\n",
    "# Iterate throw all the leagues IDs\n",
    "for i in leagues:\n",
    "    # Get the league file\n",
    "    directories = [dir for dir in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, dir)) and league_id_dir(dir,i)]\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    for folder in directories:\n",
    "        folder_path = os.path.join(root_dir, folder)\n",
    "        files = os.listdir(folder_path)\n",
    "        csv_files = [file for file in files if file.endswith('.csv')]\n",
    "    \n",
    "        for csv_file in csv_files:\n",
    "            file_path = os.path.join(folder_path, csv_file)\n",
    "            df = pd.read_csv(file_path)\n",
    "            combined_df = pd.concat([combined_df,df])\n",
    "\n",
    "    combined_df.to_csv('Fixtures/'+str(i)+'data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the fuctions for Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop the uneccecery columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Drop_columns(df):\n",
    "    # Drop the uneccecery columns\n",
    "    to_drop = ['fixture_periods_first', 'fixture_periods_second', 'fixture_timestamp', 'fixture_timezone', 'score_extratime_home', 'score_extratime_away','score_penalty_home','score_penalty_away']\n",
    "    df.drop(columns=to_drop, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coversion of the float column type to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Convert_Data_types(df):\n",
    "    columns = ['goals_home','goals_away','score_halftime_home' ,'score_halftime_away','score_fulltime_home' ,'score_fulltime_away', 'status_elapsed']\n",
    "    for col in columns:\n",
    "        df[col] = df[col].fillna(0).astype(int)\n",
    "\n",
    "    df['teams_home_winner'].fillna('0', inplace=True)\n",
    "    df['teams_away_winner'].fillna('0', inplace=True)\n",
    "\n",
    "    df['teams_home_winner'] = df['teams_home_winner'].astype(str)\n",
    "    df['teams_away_winner'] = df['teams_away_winner'].astype(str)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the date column format to YYYY-MM-DD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Convert_Date(df):\n",
    "    # Keep the date format as YYYY-MM-DD\n",
    "    df['fixture_date'] = pd.to_datetime(df['fixture_date']).dt.date\n",
    "    df.sort_values('fixture_date', inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Leagues csv files and perform the cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 78data.csv\n",
      "Processed 140data.csv\n",
      "Processed 39data.csv\n",
      "Processed 61data.csv\n",
      "Processed 135data.csv\n"
     ]
    }
   ],
   "source": [
    "files_path = '/home/hadoop/AllData/Fixtures'\n",
    "\n",
    "# List all files in the directory\n",
    "file_list = os.listdir(files_path)\n",
    "\n",
    "# Iterate through each file\n",
    "for filename in file_list:\n",
    "    file_path = os.path.join(files_path, filename)\n",
    "\n",
    "    # Read the CSV file using Pandas\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    #Drop the unwanted columns\n",
    "    df = Drop_columns(df)\n",
    "\n",
    "    #Data type conversion (from float to int)\n",
    "    df = Convert_Data_types(df)\n",
    "\n",
    "    #Convert the date format\n",
    "    df = Convert_Date(df)\n",
    "\n",
    "    # Write the modified content back to the same file\n",
    "    df.to_csv('/home/hadoop/AllData/Cleaned/'+filename, index=False)\n",
    "    print(f\"Processed {filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
